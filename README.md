# Kubernetes Cluster Deployment with Ansible Roles

## ğŸ“‹ Project Structure

```
k8s-ansible-roles/
â”œâ”€â”€ inventory.ini                  # Inventory file with all hosts
â”œâ”€â”€ group_vars/
â”‚   â”œâ”€â”€ all.yml                   # Global variables
â”‚   â””â”€â”€ k8s_masters.yml           # Master-specific variables
â”œâ”€â”€ site.yml                      # Main playbook
â”œâ”€â”€ setup-ssh.yml                 # SSH and /etc/hosts configuration
â”œâ”€â”€ create-test-pods.yml          # Test pod deployment playbook
â”œâ”€â”€ roles/
â”‚   â”œâ”€â”€ common/                   # Common preparation role
â”‚   â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ kernel-modules.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ sysctl.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ install-packages.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ setup-containerd.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ install-k8s-components.yml
â”‚   â”‚   â”‚   â””â”€â”€ reboot.yml
â”‚   â”‚   â””â”€â”€ handlers/
â”‚   â”‚       â””â”€â”€ main.yml
â”‚   â”œâ”€â”€ kubernetes/               # Kubernetes deployment role
â”‚   â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ master-init.yml
â”‚   â”‚   â”‚   â””â”€â”€ worker-join.yml
â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”‚       â””â”€â”€ kubeadm-config.yaml.j2
â”‚   â””â”€â”€ monitoring/               # Monitoring stack role
â”‚       â”œâ”€â”€ tasks/
â”‚       â”‚   â”œâ”€â”€ main.yml
â”‚       â”‚   â”œâ”€â”€ install-helm.yml
â”‚       â”‚   â”œâ”€â”€ deploy-prometheus.yml
â”‚       â”‚   â”œâ”€â”€ configure-alertmanager.yml
â”‚       â”‚   â”œâ”€â”€ deploy-alerts.yml
â”‚       â”‚   â””â”€â”€ display-info.yml
â”‚       â”œâ”€â”€ files/
â”‚       â”‚   â”œâ”€â”€ alertmanager-values.yaml
â”‚       â”‚   â””â”€â”€ high-cpu-alert.yaml
â”‚       â””â”€â”€ defaults/
â”‚           â””â”€â”€ main.yml
```

## ğŸš€ Quick Start

### Prerequisites
- 3 Ubuntu 24.04 VMs (1 master, 2 workers)
- SSH access to all nodes
- Ansible installed on control node

### Step 1: Update Inventory
Edit `inventory.ini` with your actual IP addresses:
```ini
[k8s_masters]
k8s-master ansible_host=YOUR_MASTER_IP ansible_user=ubuntu

[k8s_workers]
k8s-master ansible_host=YOUR_MASTER_IP ansible_user=ubuntu
k8s-worker1 ansible_host=YOUR_WORKER1_IP ansible_user=ubuntu
k8s-worker2 ansible_host=YOUR_WORKER2_IP ansible_user=ubuntu
```

### Step 2: Deploy Everything
```bash
# Deploy complete infrastructure (SSH setup + K8s + Monitoring)
ansible-playbook -i inventory.ini site.yml
```

### Step 3: Verify Deployment
```bash
# On master node
kubectl get nodes
kubectl get pods -A
```

## ğŸ“¦ Individual Playbook Execution

### 1. SSH Setup Only
```bash
ansible-playbook -i inventory.ini setup-ssh.yml
```
This playbook:
- Generates SSH keys on master
- Distributes keys to workers
- Configures `/etc/hosts` on all nodes
- Tests SSH connectivity

### 2. Deploy Kubernetes Only
```bash
# Deploy common preparation + Kubernetes
ansible-playbook -i inventory.ini site.yml --tags kubernetes
```

### 3. Deploy Monitoring Only
```bash
# Deploy monitoring stack (requires K8s to be running)
ansible-playbook -i inventory.ini site.yml --tags monitoring
```

### 4. Create Test Pods
```bash
ansible-playbook -i inventory.ini create-test-pods.yml
```

## ğŸ¯ Role Details

### Common Role
**Purpose**: Prepares all nodes with required dependencies

**Tasks**:
- Disable SWAP
- Configure kernel modules (overlay, br_netfilter)
- Configure sysctl parameters
- Add Docker and Kubernetes repositories
- Install containerd
- Install kubelet, kubeadm, kubectl
- Reboot nodes

### Kubernetes Role
**Purpose**: Deploys and configures Kubernetes cluster

**Master Tasks**:
- Initialize control plane with kubeadm
- Configure kubeconfig for ubuntu user
- Install Flannel CNI
- Allow pod scheduling on control plane
- Generate join command

**Worker Tasks**:
- Join workers to cluster using join command
- Verify node registration

### Monitoring Role
**Purpose**: Deploys monitoring stack with Prometheus, Grafana, and Alertmanager

**Tasks**:
- Install Helm
- Deploy kube-prometheus-stack
- Configure Alertmanager for email notifications
- Deploy custom CPU usage alerts
- Expose Grafana via NodePort

## ğŸ” SSH and Hosts Configuration

The `setup-ssh.yml` playbook automatically:

1. **Generates SSH keys** on the master node
2. **Distributes public keys** to all worker nodes
3. **Updates `/etc/hosts`** on all nodes with cluster hostnames
4. **Tests SSH connectivity** from master to workers

No manual `ssh-keygen` or `ssh-copy-id` required!

## ğŸ“Š Accessing Monitoring

After deployment, access Grafana at:
```
http://<MASTER_IP>:<GRAFANA_NODEPORT>
Username: admin
Password: admin
```

The NodePort is displayed at the end of the monitoring deployment.

## ğŸ”§ Customization

### Change Kubernetes Version
Edit `group_vars/all.yml`:
```yaml
k8s_version: "1.29"  # Change to desired version
```

### Change Pod Network CIDR
Edit `group_vars/all.yml`:
```yaml
pod_network_cidr: "10.244.0.0/16"
```

### Update Alertmanager Email
Edit `roles/monitoring/files/alertmanager-values.yaml`

### Modify Master IP
Edit `group_vars/k8s_masters.yml`:
```yaml
control_plane_endpoint: "YOUR_MASTER_IP:6443"
```

## ğŸ§ª Testing

### Verify Cluster
```bash
kubectl get nodes
kubectl get pods -A
kubectl cluster-info
```

### Test Pod Deployment
```bash
ansible-playbook -i inventory.ini create-test-pods.yml
kubectl get pods
```

### Test Monitoring
```bash
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

### Test Alerting
Generate high CPU load to trigger alert:
```bash
# On any node
stress --cpu 4 --timeout 300s
```

## ğŸ“ Notes

- Master node also acts as a worker in this lab setup
- Swap is automatically disabled on all nodes
- Containerd is configured with systemd cgroup driver
- Flannel CNI is used for pod networking
- Grafana password is set to "admin" by default
- High CPU alert triggers at >50% usage for 2 minutes

## ğŸ› Troubleshooting

### SSH Issues
```bash
# Manually verify SSH from master
ssh ubuntu@k8s-worker1
```

### Kubernetes Not Starting
```bash
# Check kubelet logs
journalctl -u kubelet -f
```

### Monitoring Pods Not Ready
```bash
# Check pod status
kubectl get pods -n monitoring
kubectl describe pod <pod-name> -n monitoring
```

### Alertmanager Email Not Working
- Verify Gmail app password is correct
- Check Alertmanager logs: `kubectl logs -n monitoring alertmanager-xxx`

## ğŸ“š Additional Resources

- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [Ansible Documentation](https://docs.ansible.com/)
- [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)

## âœ… Features Implemented

âœ… Automated SSH key distribution  
âœ… Automatic /etc/hosts configuration  
âœ… Role-based Ansible structure  
âœ… Kubernetes role (master + workers)  
âœ… Monitoring role (Prometheus + Grafana + Alertmanager)  
âœ… Modular and reusable design  
âœ… Idempotent playbooks  
âœ… Email alerting configured  
âœ… Custom CPU usage alerts  

---

**Created for Cloud Engineering Project - OpenStack Epoxy 2025.1 + Ubuntu 24.04**
